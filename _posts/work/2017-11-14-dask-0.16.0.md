---
layout: post
title: Dask Release 0.16.0
category: work
draft: true
tags: [Programming, Python, scipy, dask]
theme: twitter
---
{% include JB/setup %}

*This work is supported by [Anaconda Inc.](http://anaconda.com)
and the Data Driven Discovery Initiative from the [Moore
Foundation](https://www.moore.org/).*

I'm pleased to announce the release of Dask version 0.16.0.  This is a major
release with new features, breaking changes, stability improvements, and bug
fixes.
This blogpost outlines
notable changes since the 0.15.3 release on September 24th.

You can conda install Dask:

    conda install -c conda-forge dask

or pip install from PyPI

    pip install dask[complete] --upgrade

Conda packages are available both on conda-forge channels.  They will be on
defaults in a few days.

Full changelogs are available here:

-  [dask/dask](https://github.com/dask/dask/blob/master/docs/source/changelog.rst)
-  [dask/distributed](https://github.com/dask/distributed/blob/master/docs/source/changelog.rst)

Some notable changes follow.

Breaking Changes
----------------

-   The ``dask.async`` module was moved to ``dask.local`` for Python 3.7
    compatibility.  This was previously deprecated but is now fully removed.
-   The distributed scheduler's diagnostic JSON pages have been removed (and
    replaced by more informative templated HTML)


Dask collection interface
-------------------------

It is now easier to implement custom collections using the Dask collection
interface.

Dask collections (arrays, dataframes, bags, delayed) interact with Dask
schedulers (single-machine, distributed) with a few internal methods.  We
formalized this interface into protocols like `.__dask_graph__()` and
`.__dask_keys__()` and have published that interface
[here](http://dask.pydata.org/en/latest/custom-collections.html).  Any object
that implements the methods described in that document will interact will all
dask scheduler features as a first-class Dask object.

```python
class MyDaskCollection(object):
    def __dask_graph__(self):
        ...

    def __dask_keys__(self):
        ...

    def __dask_optimize__(self):
        ...

    ...
```

This interface has already been implemented within the XArray project for
labeled and indexed arrays.  Now all XArray classes (DataSet, DataArray,
Variable) are fully understood by all Dask schedulers.  They are as first-class
as dask.arrays or dask.dataframes.

```python
import xarray as xa
from dask.distributed import Client

client = Client()

ds = xa.open_mfdataset('*.nc', ...)

ds = client.persist(ds)  # XArray object integrate seemlessly with Dask schedulers
```

-  Documentatation:
   [http://dask.pydata.org/en/latest/custom-collections.html](http://dask.pydata.org/en/latest/custom-collections.html)

*Work on Dask's collection interfaces was primarily done by Jim Crist*


Tornado 5 compatibility
-----------------------

In an effort to improve inter-worker bandwidth on exotic hardware (24Gb/s
networks) Dask developers are proposing changes to the Tornado library for
concurrent network programming (Dask is built on Tornado).

However, in order to use these changes Dask itself needs to run on the next
version of Tornado, Tornado 5.0.0 (in development), which breaks a number of
interfaces on which Dask has relied.  Dask developers have been resolving these
and we encourage other PyData developers to do the same (for example neither
Bokeh nor Jupyter work on Tornado 5.0.0-dev).

*Network performance and Tornado compatibility are primarily being handled by
Antoine Pitrou*


Parquet Compatibility
---------------------

Dask.dataframe can use either of the two common Parquet libraries in Python,
Apache Arrow and Fastparquet.  Each library has different strengths and tends
to be preferred by different users.  We've significantly extended Dask's
parquet test suite to cover each library, including roundtrip compatibility.
Notably, you can now both read and write with PyArrow.

```python
df.to_parquet('...', engine='pyarrow')
df = dd.read_parquet('...', engine='fastparquet')
```

*This work was jointly done by Uwe Korn, Jim Crist, and Martin Durant*


Retrying Tasks
--------------

One of the most requested features in the Dask.distributed scheduler is the
ability to retry failed tasks.  This is particularly useful to people using
Dask as a task queue, rather than as a big dataframe or array.

```python
future = client.submit(func, *args, retries=5)
```

*Task retries were primarily built by Antoine Pitrou*


Transactional Work Stealing
---------------------------

The Dask.distributed task scheduler performs load balancing through work
stealing.  Now this stealing is transactional, meaning that it will avoid
accidentally running the same task twice on different machines. This behavior
is especially important for people using Dask tasks for side effects.

It is still possible for the same task to run twice, but now this only happens
in more extreme situations, such as when a worker dies or a TCP connection is
severed (neither of which are common).

*Transactional work stealing was primarily implemented by Matthew Rocklin*


New Diagnostic Pages
--------------------

There is a new set of diagnostic web pages available in the *Info* tab of the
dashboard.  These pages provide more in-depth information about each worker and
task, but are not dynamic in any way.  They use Tornado templates rather than
Bokeh plots, which means that they are less responsive but are much easier to
build.  If people want to expose more scheduler state this is an easy and cheap
way to do so.

<img src="{{BASE_PATH}}/images/scheduler-info-task.png"
     alt="Task page of Dask's scheduler info dashboard"
     width="60%">

-  [Existing templates](https://github.com/dask/distributed/tree/master/distributed/bokeh/templates)


Nested compute calls
--------------------

Calling `.compute()` *within* a task now invokes the same distributed
scheduler.  This enables writing more complex workloads with less though to
starting worker clients.

```python
import dask
from dask.distributed import Client
client = Client()  # only works for the newer scheduler

@dask.delayed
def f(x):
    ...
    return dask.compute(...)  # can call dask.compute within delayed task

dask.compute([f(i) for ...])
```

*Nested compute calls were primarily done by Matthew Rocklin with Olivier
Grisel for advice.*


More aggressive Garbage Collection
----------------------------------

The workers now explicitly call `gc.collect()` at various times when under
memory pressure and when releasing data.  This helps to avoid some memory
leaks, especially when using Pandas dataframes.  Doing this carefully has
proven to be surprisingly detail-oriented.

*Improved garbage collection was primarily implemented and tested by Fabian
Keller and Olivier Grisel, with recommendations by Antoine Pitrou*


Related projects
----------------

### Dask-ML

A variety of Dask Machine Learning projects are now being assembled under one
unified repository, [dask-ml](http://dask-ml.readthedocs.io/en/latest/).  We
encourage users and researchers alike to read through that project.  There are,
we believe, many useful and interesting approaches contained within.

- Docs: [dask-ml.readthedocs.io](http://dask-ml.readthedocs.io/en/latest/)
- Github: [github.com/dask/dask-ml](https://github.com/dask/dask-ml)

*The work to assmeble and curate these algorithms is primarily being handled by
Tom Augspurger*


### XArray

The [XArray](http://xarray.pydata.org/en/stable/) project for indexed and
labeled arrays is also releasing their major 0.10.0 release this week, which
includes many performance improvements, particularly for using Dask on larger
datasets.

-  Docs: [http://xarray.pydata.org/en/stable/](http://xarray.pydata.org/en/stable/)
-  Release notes: [TODO](TODO)


Acknowledgements
----------------

The following people contributed to the dask/dask repository since the 0.15.3
release on September 24th

-


The following people contributed to the dask/distributed repository since the
1.19.1 release on September 24nd:

-
