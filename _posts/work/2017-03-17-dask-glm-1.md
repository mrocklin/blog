---
layout: post
title: Dask-GLM development
draft: true
category: work
tags: [Programming, Python, scipy]
theme: twitter
---
{% include JB/setup %}

*This work is supported by [Continuum Analytics](http://continuum.io)
the [XDATA Program](http://www.darpa.mil/program/XDATA)
and the Data Driven Discovery Initiative from the [Moore
Foundation](https://www.moore.org/).*

Summary
-------

We discuss building distributed optimization algorithms with Dask.  We show
both some simple examples and benchmarks.  We also talk about the experience of
learning Dask to do this kind of work.

This blogpost is co-authored by Chris White (Capital One) who knows
optimization and Matthew Rocklin (Continuum Analytics) who knows distributed
computing.


Introduction
------------

Many machine learning and statistics algorithms like logistic regression depend
on convex optimization algorithms like Newton's method, stochastic gradient
descent, and others.  These optimization algorithms are both pragmatic (they're
used in many applications) and mathematically interesting.  As a result these
algorithms have been the subject of study by researchers and graduate students
around the world for years both in academia and in industry.

Things got interesting about five or ten years ago when datasets grew beyond
the size of working memory and "Big Data" became a buzzword.  Now parallel and
distributed solutions for these algorithms have become the norm, and a
researcher's skillset now has to extend beyond linear algebra and optimization
theory to include parallel algorithms and possibly even network programming,
especially if you want to explore and create more interesting algorithms.

1.  Algorithmic researcher (Chris): someone who knows optimization and
    iterative algorithms like ,  (TODO: Chris fill in) but isn't so hot on
    distributed computing topics like sockets, MPI, load balancing, etc..
2.  Distributed systems/Dask developer (Matt): someone who knows how to move
    bytes around and keep machines busy but doesn't know the right way to do a
    line search or handle a poorly conditioned matrix


Prototyping Algorithms in Dask
--------

Given knowledge of algorithms and of NumPy array computing it is easy to write parallel algorithms with Dask.   For a range of complicated algorithmic structures we have two straightforward choices:

1.  Exploit naive parallelism in common operations such as matrix multiplication
2.  Parallel process the data in "chunks" and use those outputs to update each iteration

Coding up either of these options from scratch can be a daunting task, but with Dask it is actually as simple as writing NumPy code.  

Let's build up an example of fitting a large linear regression model using both built-in array parallellism and fancier, more customized parallelization features that Dask offers.  The `dask.array` module helps us to easily parallelize standard NumPy functionality using the same syntax -- we'll start there.

#### Data Creation

Dask has [many ways of creating dask arrays](http://dask.pydata.org/en/latest/array-creation.html); to get us started quickly prototyping let's create some random data in a way that should look familiar to NumPy users.

```python
import dask
import dask.array as da
import numpy as np

from dask import delayed, persist, compute
from dask.distributed import Client

client = Client()

## create inputs with a bunch of independent normals
beta = np.random.random(100) # random beta coefficients, no intercept
X = da.random.normal(0, 1, size=(1000000, 100), chunks=(100000, 100))
y = X.dot(beta) + da.random.normal(0, 1, size=1000000, chunks=(100000,))

## make sure all chunks are ~equally sized
client.rebalance([X, y])
```

Observe that `X` is a dask array stored in 10 chunks, each of size `(1000, 100)`.  Also note that `X.dot(beta)` runs smoothly for both `numpy` and `dask` arrays, so we can write code that basically works in either world.

**Caveat:**  If `X` is a `numpy` array and `beta` is a `dask` array, `X.dot(beta)` will output an *in-memory* `numpy` array.  This is usually not desirable as you want to carefully choose when to load something into memory. One fix is to use `multipledispatch` to handle odd edge cases; for a starting example, check out the `dot` code [here](https://github.com/dask/dask-glm/blob/master/dask_glm/utils.py#L65-L84).

Dask also has convenient visualization features built in that we will leverage; below we visualize our data in its 10 independent chunks:

*INSERT X IMAGE*

#### Array Programming

*If you can write iterative array-based algorithms in NumPy, you can write iterative parallel algorithms in Dask*

As we've already seen, Dask inherits much of the NumPy API that we are familiar with, so we can write simple NumPy-style iterative optimization algorithms that will leverage the parallelism `dask.array` has built-in already.  For example, if we want to naively fit a linear regression model on the data above, we are trying to solve the following convex optimization problem:
$$
\min_{\beta} \|y - X\beta\|_2^2
$$

Recall that in non-degenerate situations this problem has a closed-form solution that is given by:
$$
\beta^* = \left(X^T X\right)^{-1} X^T y
$$

We can compute $\beta^*$ using the above formula with Dask:

```python
## naive solution
beta_star = da.linalg.solve(X.T.dot(X), X.T.dot(y))
>>> np.absolute(beta_star.compute() - beta).max()
0.0024817567237768179
```

Sometimes matrix inversion is too costly, and we want to solve the above problem using only simple matrix-vector multiplications.  To this end, let's take this one step further and actually implement a gradient desecent algorithm which exploits parallel matrix operations.  Recall that gradient descent iteratively refines an initial estimate of beta via the update:
$$
\beta^+ = \beta - \alpha \nabla f(\beta)
$$

where $\alpha$ can be chosen based on a number of different "step-size" rules; for the purposes of exposition, we will stick with a constant step-size:

```python
## quick step-size calculation to guarantee convergence
_, s, _ = da.linalg.svd_compressed(2 * X.T.dot(X), k=1)
step_size = 1 / s.compute() - 1e-8

## define some parameters
max_steps = 100
tol = 1e-8
beta_hat = np.zeros(100) # initial guess
X, y = persist(X, y)

for k in range(max_steps):
    Xbeta = X.dot(beta_hat)
    func = ((y - Xbeta)**2).sum()
    gradient = 2 * X.T.dot(Xbeta - y)

## update
    obeta = beta_hat
    beta_hat = beta_hat - step_size * gradient
    new_func = ((y - X.dot(beta_hat))**2).sum()
    beta_hat, func, new_func = compute(beta_hat, func, new_func) # the only dask

## check for convergence
    change = np.absolute(beta_hat - obeta).max()

    if change < tol:
        break
                                
>>> np.absolute(beta_hat - beta).max()
0.0024817567259038942
```

**TODO**: Comment on any dask-ifications

To better appreciate all the scheduling that is happening in one single step of the above algorithm, here is a visualization of the computation necessary to compute `beta_hat` and the new function value `new_func`:

**INSERT IMAGE HERE**

#### Array Programming + dask.delayed

Now that we've seen how to use the built-in parallelism offered by Dask, let's go one step further and talk about writing more customized parallel algorithms; many distributed "consensus" based algorithms in machine learning are based on the idea that each chunk of data can be "processed" in parallel, and send their guess for the optimal parameter value to some master node.  The master then computes a "consensus" estimate for the optimal parameters and reports that back to all of the workers.  Each worker then processes their chunk of data given this new information, and the process continues until convergence.

One such algorithm is the [Alternating Direction Method of Multipliers](http://stanford.edu/~boyd/admm.html), or ADMM for short.  For the sake of this post, we will consider the work done by each worker to be a black box.

We will also be considering a *regularized* version of the problem above, namely:

$$
\min_{\beta} \|y - X\beta\|_2^2 + \lambda \|\beta\|_1
$$

At the end of the day, all we will do is:
- create NumPy functions which define how each chunk updates its parameter estimates
- wrap those functions in `dask.delayed`
- call `dask.compute` and process the individual estimates, again using NumPy

First we need to define some "local" functions that the chunks will use to update their individual parameter estimates, and import the black box `local_update` step from `dask_glm`; also, we will need the so-called "shrinkage" operator (which is the [proximal operator](https://en.wikipedia.org/wiki/Proximal_operator) for the $l1$-norm in our problem):

```python
from dask_glm.algorithms import local_update

def local_f(beta, X, y, z, u, rho):
    return ((y - X.dot(beta)) **2).sum() + (rho / 2) * np.dot(beta - z + u, 
                                                                  beta - z + u)

def local_grad(beta, X, y, z, u, rho):
    return 2 * X.T.dot(X.dot(beta) - y) + rho * (beta - z + u)


def shrinkage(beta, t):
    return np.maximum(0, beta - t) - np.maximum(0, -beta - t)

## set some algorithm parameters
max_steps = 10
lamduh = 7.2
rho = 1.0

(n, p) = X.shape
nchunks = X.npartitions
XD = X.to_delayed().flatten().tolist() # imagine a list of numpy arrays, one for each chunk
yD = y.to_delayed().flatten().tolist()

# the initial consensus estimate
z = np.zeros(p)

# an array of the individual "dual variables" and parameter estimates,
# one for each chunk of data
u = np.array([np.zeros(p) for i in range(nchunks)])
betas = np.array([np.zeros(p) for i in range(nchunks)])

for k in range(1):
        
# process each chunk in parallel, using the black-box 'local_update' magic
    new_betas = [delayed(local_update)(xx, yy, bb, z, uu, 
    rho, f=local_f, fprime=local_grad) for
    xx, yy, bb, uu in zip(XD, yD, betas, u)]
    img = dask.visualize(dask.delayed(update_global_estimate)(new_betas))
    new_betas = np.array(da.compute(*new_betas))

# everything else is NumPy code occuring at "master"
    beta_hat = 0.9 * new_betas + 0.1 * z

# create "consensus" estimate
    zold = z.copy()
    ztilde = np.mean(beta_hat + np.array(u), axis=0)
    z = shrinkage(ztilde, lamduh / (rho * nchunks))

# update dual variables
    u += beta_hat - z

>>> print('Number of coefficients 0\'ed out:\n' + '='*35)
>>> print((z == 0).sum())

Number of coefficients 0'ed out:
===================================
12
```

There is of course a little bit more work occuring in the above algorithm, but it should be clear that the distributed operations are *not* one of the difficult pieces.

**INSERT IMAGE HERE**


Experiment
----------

*We compare dask-glm implementations against Scikit-learn on a laptop, and then
show them running on a cluster.*

*Reproducible notebook is available here: TODO*

We've already done the work above, and more, in the nascent [dask-glm
project](https://github.com/dask/dask-glm).  This project has convex
optimization algorithms for gradient descent, proximal gradient descent,
newton's method, and ADMM.  These implementations extend the implementations
above by also thinking about stopping criteria and other niceties that we
avoided above for simplicity.

how off these algorithms we're going to perform a simple numerical experiment
that compares the numerical performance of proximal gradient descent and ADMM
alongside Scikit-Learn's LogisiticRegression and SGD implementations on a
single machine (a personal laptop) and then follows up by scaling the dask-glm
options to a moderate cluster.  As a disclaimer, these experiments will be
somewhat crude.  We're using artificial data, we're not tuning parameters or
even finding parameters at which these algorithms are producing results of the
same accuracy.  The goal of this section is just to give a general feeling of
how things compare.

We create data

```python
## size of problem (no. observations)
N = 8e6
chunks = 1e6
seed = 20009
beta = (np.random.random(15) - 0.5) * 3

X = da.random.random((N,len(beta)), chunks=chunks)
y = make_y(X, beta=np.array(beta), chunks=chunks)

X, y = persist(X, y)
client.rebalance([X, y])
```

And run each of our algorithms as follows:

```python
# Dask-GLM Proximal Gradient
proximal_grad(X, y, lamduh=alpha)

# Dask-GLM ADMM
X2 = X.rechunk((1e5, None)).persist()  # ADMM prefers smaller chunks
y2 = y.rechunk(1e5).persist()
result = admm(X2, y2, lamduh=alpha)

# Scikit-Learn LogisticRegression
nX, ny = compute(X, y)  # sklearn wants numpy arrays
LogisticRegression(penalty='l1', C=1).fit(nX, ny).coef_

# Scikit-Learn Stochastic Gradient Descent
SGDClassifier(loss='log',
              penalty='l1',
              l1_ratio=1,
              n_iter=10,
              fit_intercept=False).fit(nX, ny).coef_
```

We then compare with the $L_{\infty}$ norm (largest different value).

```python
abs(result - beta).max()
```

Times and accuracies for these parameters are shown in the table below:

<table>
<thead><tr>
  <th>Algorithm</th>
  <th>Error</th>
  <th>Duration (s)</th>
</tr></thead>
<tbody>
<tr>
  <td>Proximal Gradient</td>
  <td>0.0227</td>
  <td>128</td>
</tr>
<tr>
  <td>ADMM</td>
  <td>0.0125</td>
  <td>34.7</td>
</tr>
<tr>
  <td>LogisticRegression</td>
  <td>0.0132</td>
  <td>79</td>
</tr>
<tr>
  <td>SGDClassifier</td>
  <td>0.0456</td>
  <td>29.4</td>
</tr>
</tbody>
<table>

Again, please don't take these numbers too seriously.  TODO: Chris add
disclaimers.  Also, Dask-glm is using a full four-core laptop while SKLearn is
restricted to use a single core.


TODO Matt: Add profile plots
