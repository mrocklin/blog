---
layout: post
title: Dask on HPC - Initial Work
category: work
draft: true
tags: [Programming, Python, scipy, dask, pangeo]
theme: twitter
---
{% include JB/setup %}

*This work is supported by [Anaconda Inc.](http://anaconda.com)
and the NSF EarthCube program.*

As mentioned in [this announcement]() Dask developers are collaborating
with atmospheric and oceanographic scientists to scale Dask/XArray computations
on big-iron super-computers.  The [full text]() of the proposed work is
[available here]().  We are very grateful to the NSF EarthCube program for
funding.

I've worked on this problem for about the past week.  This blogpost details
ongoing and completed efforts.  It includes links where you can learn more and
hopefully get involved.  We're going to cover the following topics:

1.  Deploying Dask on HPC systems with MPI
2.  Interactive deployments on a batch job scheduler
3.  The virtues of JupyterLab in a remote system
4.  Network performance
5.  Modernizing XArray's interactions with Dask schedulers

A video walkthrough deploying Dask on XArray on an HPC system is available
[here](https://www.youtube.com/watch?v=7i5m78DSr34).


Deployment
----------

HPC systems use job schedulers including SGE, SLURM, PBS, LSF, and others.  Dask
has been deployed on all of these systems before either by academic groups or
financial companies.  However every time we do this it's a little different and
generally tailored to a particular cluster.

We wanted to make something more general.  This started out as a [github issue
on PBS scripts](https://github.com/dask/distributed/issues/1260) that tried to
make a simple common template that people could copy-and-modify.
Unfortunately, there were significant challenges with this.

A slightly more successful approach has been to bootstrap Dask off of an
[MPI4Py](http://mpi4py.readthedocs.io/en/stable/) program.  MPI is well
supported and (more importantly) consistently supported by all job schedulers,
so depending on this provides a level of stability.  Now dask.distributed ships
with a new `dask-mpi` executable:

    mpirun --np 4 dask-mpi

To be clear, Dask isn't using MPI for inter-process communication.  It's still
using TCP.  We're just using MPI to launch a scheduler and several workers and
hook them all together.

Socially this is useful because *every* cluster management team knows how to
support MPI, so anyone with access to such a cluster has someone they can ask
for help.


Working Interactively on a Batch Scheduler
------------------------------------------

Our collaboration is focused on interactive analysis of big datasets.  This
means that people expect to open up Jupyter notebooks, connect to clusters
of many machines, and compute on those machines while they sit at their
computer.

<img src="{{ BASE_PATH }}/images/pangeo-dask-client.png" width="60%">

Unfortunately most job schedulers were designed for batch scheduling.  They
will try to run your job quickly, but don't mind waiting for a few hours for a
nice slot to open up.  As you ask for more time and more machines, waiting times
can increase drastically.  For most MPI jobs this is fine, as people aren't
expecting to get a result right away and they're certainly not interacting with
the program, but in our case we really do want some results right away, even if
they're only part of what we asked for.

Handling this problem long term will require both technical work and policy
decisions.  In the short term we can take advantage of two facts:

1.  Many small jobs can get started more quickly than a few large ones.  These
    take advantage of holes in the schedule that are too small to be used by
    larger jobs.
2.  Dask doesn't need to be started all at once.  Workers can come and go.

And so I find that if I ask for several single machine jobs I can easily cobble
together a sizable cluster that starts very quickly.  In practice this looks
like the following:

```
$ qsub start-dask.sh
$ qsub add-one-worker.sh
$ qsub add-one-worker.sh
$ qsub add-one-worker.sh
$ qsub add-one-worker.sh
$ qsub add-one-worker.sh
$ qsub add-one-worker.sh
```

My main job has a wall time of about an hour.  The workers have shorter wall
times.  I can add more as needed throughout my computation.


Jupyter Lab and Web Frontends
-----------------------------

I've been starting a Jupyter server on my main Dask scheduler node and then
ssh-tunneling so that I can use that Jupyter server from my home computer.

In particular I've been using Jupyter Lab.  This is both because I prefer the
notebook experience in Jupyter Lab and also because it allows me to create more
terminals, navigate the file system, and so on, all without having to open
several more connections.

Jupyter Lab is *much* more valuable when every connection to a remote machine
is somewhat painful to set up.  This one connection really gives you just about
everything you need.

I have set up a script that starts Jupyter Lab on my cluster allocation and
prints out the correct SSH-tunneling script for me to copy-paste.  It works
like a charm.

```python
from dask.distributed import Client
client = Client(scheduler_file='scheduler.json')

import socket
host = client.run_on_scheduler(socket.gethostname)

def start_jlab(dask_scheduler):
    import subprocess
    proc = subprocess.Popen(['jupyter', 'lab', '--ip', host, '--no-browser'])
    dask_scheduler.jlab_proc = proc

client.run_on_scheduler(start_jlab)

print("ssh -N -L 8787:%s:8787 -L 8888:%s:8888 -L 8789:%s:8789 cheyenne.ucar.edu" % (host, host, host))
```

Network Performance
-------------------

On this cluster Dask gets about 1GB/s simultaneous read/write network bandwidth
per machine.  For any commodity or cloud-based system this is *very fast*.
However for a super-computer this is only about 30% of peak (see [hardware
specs](https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne)).

I suspect that this is due to byte-handling in Tornado, the networking library
that Dask uses under the hood.  The following image shows the diagnostic
dashboard for one worker after a communication-heavy workload.  We see 1GB/s
for both read and write.  We also see 100% CPU usage.

<a href="{{BASE_PATH}}/images/pangeo-network.png"><img src="{{BASE_PATH}}/images/pangeo-network.png" width="60%"></a>

Network performance is a big question for HPC users looking at Dask.  If we can
get near MPI bandwidth then that will be interesting for this
performance-oriented community.

-  [Github issue for this project](https://github.com/pangeo-data/pangeo-discussion/issues/6)
-  [Github issue for Tornado](https://github.com/tornadoweb/tornado/issues/2147)


XArray and Dask.distributed
---------------------------

XArray was the first major project to use Dask internally.  This early
integration was critical to prove out Dask's internals with user feedback.
However it also means that some parts of XArray were designed well before some
of the newer parts of Dask, notably the asynchronous distributed scheduling
features.

XArray can still use Dask on a distributed cluster, but only with the subset of
features that are also available with the single machine scheduler.  This means
that persisting data in distributed RAM, parallel debugging, publishing shared
datasets, and so on all require significantly more work today with XArray than
they should.

To address this we plan to update XArray to follow a newly proposed [Dask
interface](https://github.com/dask/dask/pull/1068#issuecomment-326591640).
This is complex enough to handle all Dask scheduling features, but light weight
enough not to actually require any dependence on the Dask library itself.
(Work by [Jim Crist](http://jcrist.github.io/).)

We will also eventually need to look at reducing overhead for inspecting
several NetCDF files, but we haven't yet run into this, so I plan to wait.


More information
----------------

If you would like to track progress we're keeping everything within the
[Pangeo GitHub issue tracker](https://github.com/pangeo-data/pangeo-discussion/issues).
If you use these technologies then we would love to interact more to ensure
that this work has broad impact across scientific groups.
