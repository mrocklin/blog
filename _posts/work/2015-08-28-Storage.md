---
layout: post
title: Efficient Tabular Storage
category : work
draft: true
tags : [scipy, Python, Programming, dask, blaze]
---
{% include JB/setup %}

**tl;dr: We discuss efficient techniques for on-disk storage of tabular
data, notably the following:**

* Binary stores
* Column stores
* Categorical support
* Compression
* Indexed/Partitioned stores

**We use NYCTaxi dataset for examples, and introduce a small project,
[Castra](https://github.com/blaze/castra).**

*This work is supported by [Continuum Analytics](http://continuum.io)
and the [XDATA Program](http://www.darpa.mil/program/XDATA)
as part of the [Blaze Project](http://blaze.pydata.org)*


Larger than Memory Data and Disk I/O
------------------------------------

We analyze large datasets (10-100GB) on our laptop by extending memory with
disk.  Tools like [dask.array](http://dask.pydata.org/en/latest/array.html) and
[dask.dataframe](http://dask.pydata.org/en/latest/dataframe.html) make this
easier for array and tabular data.

Interaction times can improve significantly (from minutes to seconds) if we
choose to store our data on disk efficiently.  This is particularly important
for large data because we can no longer separately "load in our data" while we
get a coffee and then iterate rapidly on our dataset once it's comfortably
in memory.

*Larger-than-memory datasets force our interactive workflows to include the
hard drive.*


CSV is convenient but slow
--------------------------

CSV is great.  It's human readable, accessible by every tool (even Excel!), and
pretty simple.

CSV is also slow.  The `pandas.read_csv` parser maxes out at 100MB/s
on simple data.  This doesn't include any keyword arguments like datetime
parsing that might slow it down further.  Consider the time to parse a 24GB
dataset:

    24GB / (100MB/s) == 4 minutes

A four minute delay is too long for interactivity.  We need to operate in
seconds rather than minutes otherwise people leave to work on something else.
This improvement from a few minutes to a few seconds is entirely possible if we
choose better formats.


Example with CSVs
-----------------

As an example lets play with the NYC Taxi dataset using
[dask.dataframe](http://http://dask.pydata.org/en/latest/dataframe.html) a
library that copies the Pandas API but operates in chunks off of disk.

{% highlight Python %}
>>> import dask.dataframe as dd

>>> df = dd.read_csv('csv/trip_data_*.csv',
...                  skipinitialspace=True,
...                  parse_dates=['pickup_datetime', 'dropoff_datetime'])

>>> df.head()
{% endhighlight %}

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>medallion</th>
      <th>hack_license</th>
      <th>vendor_id</th>
      <th>rate_code</th>
      <th>store_and_fwd_flag</th>
      <th>pickup_datetime</th>
      <th>dropoff_datetime</th>
      <th>passenger_count</th>
      <th>trip_time_in_secs</th>
      <th>trip_distance</th>
      <th>pickup_longitude</th>
      <th>pickup_latitude</th>
      <th>dropoff_longitude</th>
      <th>dropoff_latitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>89D227B655E5C82AECF13C3F540D4CF4</td>
      <td>BA96DE419E711691B9445D6A6307C170</td>
      <td>CMT</td>
      <td>1</td>
      <td>N</td>
      <td>2013-01-01 15:11:48</td>
      <td>2013-01-01 15:18:10</td>
      <td>4</td>
      <td>382</td>
      <td>1.0</td>
      <td>-73.978165</td>
      <td>40.757977</td>
      <td>-73.989838</td>
      <td>40.751171</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>
      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>
      <td>CMT</td>
      <td>1</td>
      <td>N</td>
      <td>2013-01-06 00:18:35</td>
      <td>2013-01-06 00:22:54</td>
      <td>1</td>
      <td>259</td>
      <td>1.5</td>
      <td>-74.006683</td>
      <td>40.731781</td>
      <td>-73.994499</td>
      <td>40.750660</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>
      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>
      <td>CMT</td>
      <td>1</td>
      <td>N</td>
      <td>2013-01-05 18:49:41</td>
      <td>2013-01-05 18:54:23</td>
      <td>1</td>
      <td>282</td>
      <td>1.1</td>
      <td>-74.004707</td>
      <td>40.737770</td>
      <td>-74.009834</td>
      <td>40.726002</td>
    </tr>
    <tr>
      <th>3</th>
      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>
      <td>51EE87E3205C985EF8431D850C786310</td>
      <td>CMT</td>
      <td>1</td>
      <td>N</td>
      <td>2013-01-07 23:54:15</td>
      <td>2013-01-07 23:58:20</td>
      <td>2</td>
      <td>244</td>
      <td>0.7</td>
      <td>-73.974602</td>
      <td>40.759945</td>
      <td>-73.984734</td>
      <td>40.759388</td>
    </tr>
    <tr>
      <th>4</th>
      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>
      <td>51EE87E3205C985EF8431D850C786310</td>
      <td>CMT</td>
      <td>1</td>
      <td>N</td>
      <td>2013-01-07 23:25:03</td>
      <td>2013-01-07 23:34:24</td>
      <td>1</td>
      <td>560</td>
      <td>2.1</td>
      <td>-73.976250</td>
      <td>40.748528</td>
      <td>-74.002586</td>
      <td>40.747868</td>
    </tr>
  </tbody>
</table>

### Time Costs

It takes a second to load the first few lines but 11 to 12 minutes to roll
through the entire dataset.  We make a pretty picture below of a random sample
of the taxi pickup locations in New York City.  This example is taken from a
[full example notebook here](https://github.com/blaze/dask-examples/blob/master/nyctaxi-storage.ipynb).

{% highlight Python %}
df2 = df[(df.pickup_latitude > 40) &
         (df.pickup_latitude < 42) &
         (df.pickup_longitude > -75) &
         (df.pickup_longitude < -72)]

sample = df2.sample(frac=0.0001)
pickup = sample[['pickup_latitude', 'pickup_longitude']]

result = pickup.compute()

from bokeh.plotting import figure, show, output_notebook
p = figure(title="Pickup Locations")
p.scatter(result.pickup_longitude, result.pickup_latitude, size=3, alpha=0.2)
{% endhighlight %}

<iframe src="https://cdn.rawgit.com/mrocklin/e269d02ed15947ba58c5/raw/2c8620c11af14fcdd6e0948de1d6b398cf56001a/nyc-trip-pickup.html"
        width="640" height="450"></iframe>

### Eleven minutes is a long time

This result takes eleven minutes to compute, almost all of which is parsing CSV
files.  While this may be acceptable for a single computation we invariably
make mistakes and start over or find new avenues in our data to explore.

Interactive exploration of larger-than-memory datasets requires us to evolve
beyond CSV files.


Principles to store tabular data
--------------------------------

*What efficient techniques exist for tabular data?*

A good solution may have the following attributes:

1.  Binary
2.  Columnar
3.  Categorical support
4.  Compressed
5.  Indexed/Partitioned

We discuss each of these below.


### Binary

Consider the text '1.23' as it is stored in a CSV file and how it is stored as
a Python/C float in memory:

*  CSV:  `1.23`
*  C/Python float: `0x3f9d70a4`

These look *very different*.  When we load `1.23` from a CSV textfile
we need to translate it to `0x3f9d70a4`; this takes time.

A binary format stores our data on disk exactly how it will look in memory; we
store the bytes `0x3f9d70a4` directly on disk so that when we load data
from disk to memory no extra translation is necessary.  Our file is no longer
human readable but it's much faster.

This gets more intense when we consider datetimes:

*  CSV: 2015-08-25 12:13:14
*  NumPy datetime representation: 1440529994000000  (as an integer)

Every time we parse a datetime we need to compute how many microseconds it has
been since the epoch.  This calculation needs to take into account things like
how many days in each month, and all of the intervening leap years.  This is
slow.  A binary representation would record the integer directly on disk (as
`0x51e278694a680`) so that we can load our datetimes directly into memory
without calculation.


### Columnar

Many analytic computations only require a few columns at a time, often only
one, e.g.

{% highlight Python %}
>>> df.passenger_counts.value_counts().compute().sort_index()
0           3755
1      119605039
2       23097153
3        7187354
4        3519779
5        9852539
6        6628287
7             30
8             23
9             24
129            1
255            1
Name: passenger_count, dtype: int64
{% endhighlight %}

Of our 24 GB we may only need 2GB.  *Columnar* storage means storing each
column separately from the others so that we can read relevant columns
without passing through irrelevant columns.

Our CSV example fails at this.  While we only want two columns,
`pickup_datetime` and `pickup_longitude`, we pass through all of our data to
collect the relevant fields.  The pickup location data is mixed with all the
rest.


### Categoricals

*Categoricals encode repetitive text columns (normally very expensive) as
integers (very very cheap) in a way that is invisible to the user.*

Consider the following (mostly text) columns of our NYC taxi dataset:

{% highlight Python %}
>>> df[['medallion', 'vendor_id', 'rate_code', 'store_and_fwd_flag']].head()
{% endhighlight %}

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>medallion</th>
      <th>vendor_id</th>
      <th>rate_code</th>
      <th>store_and_fwd_flag</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>89D227B655E5C82AECF13C3F540D4CF4</td>
      <td>CMT</td>
      <td>1</td>
      <td>N</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>
      <td>CMT</td>
      <td>1</td>
      <td>N</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>
      <td>CMT</td>
      <td>1</td>
      <td>N</td>
    </tr>
    <tr>
      <th>3</th>
      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>
      <td>CMT</td>
      <td>1</td>
      <td>N</td>
    </tr>
    <tr>
      <th>4</th>
      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>
      <td>CMT</td>
      <td>1</td>
      <td>N</td>
    </tr>
  </tbody>
</table>

Each of these columns represents elements of a small set:

*  There are **two** vendor ids
*  There are **twenty one** rate codes
*  There are **three** store-and-forward flags (Y, N, missing)
*  There are about **13000** taxi medallions. (still a small number)

And yet we store these elements in large and cumbersome dtypes:

{% highlight Python %}
In [4]: df[['medallion', 'vendor_id', 'rate_code', 'store_and_fwd_flag']].dtypes
Out[4]:
medallion             object
vendor_id             object
rate_code              int64
store_and_fwd_flag    object
dtype: object
{% endhighlight %}

We use `int64` for rate code, which could easily have fit into an `int8` an
opportunity for an 8x improvement in memory use.  The object dtype used for
strings in Pandas and Python takes up a lot of memory and is quite slow:

{% highlight Python %}
In [1]: import sys
In [2]: sys.getsizeof('CMT')  # bytes
Out[2]: 40
{% endhighlight %}

Categoricals replace the original column with a column of integers (of the
appropriate size, often `int8`) along with a small index mapping those integers to the
original values.  I've [written about categoricals
before](http://matthewrocklin.com/blog/work/2015/06/18/Categoricals/) so I
won't go into too much depth here.  Categoricals increase both storage and
computational efficiency by about 10x if you have text data that describes
elements in a category.


### Compression

After we've encoded everything well and separated our columns we find ourselves
limited by disk I/O read speeds.  Disk read bandwidths range from 100MB/s
(laptop spinning disk hard drive) to 2GB/s (RAID of SSDs).  This read speed
strongly depends on how large our reads are.  The bandwidths given above
reflect large sequential reads such as you might find when reading all of a
100MB file in one go. Performance degrades for smaller reads.  Fortunately, for
analytic queries we're often in the large sequential read case (hooray!)

We reduce disk read times through compression.  Consider the datetimes of the
NYC taxi dataset.  These values are repetitive and slowly changing; a perfect match for modern compression techniques.

{% highlight Python %}
>>> ind = df.index.compute()  # this is on presorted index data (see castra section below)
>>> ind
DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 00:00:00',
               '2013-01-01 00:00:00', '2013-01-01 00:00:00',
               '2013-01-01 00:00:00', '2013-01-01 00:00:00',
               '2013-01-01 00:00:00', '2013-01-01 00:00:00',
               '2013-01-01 00:00:00', '2013-01-01 00:00:00',
               ...
               '2013-12-31 23:59:42', '2013-12-31 23:59:47',
               '2013-12-31 23:59:48', '2013-12-31 23:59:49',
               '2013-12-31 23:59:50', '2013-12-31 23:59:51',
               '2013-12-31 23:59:54', '2013-12-31 23:59:55',
               '2013-12-31 23:59:57', '2013-12-31 23:59:57'],
               dtype='datetime64[ns]', name=u'pickup_datetime', length=169893985, freq=None, tz=None)
{% endhighlight %}


#### Benchmark datetime compression

We can use a modern compression library, like `fastlz` or [blosc](http://blosc.org/) to compress this data at high speeds.

{% highlight Python %}
In [36]: import blosc

In [37]: %time compressed = blosc.compress_ptr(address=ind.values.ctypes.data,
    ...:                                       items=len(ind),
    ...:                                       typesize=ind.values.dtype.alignment,
    ...:                                       clevel=5)
CPU times: user 3.22 s, sys: 332 ms, total: 3.55 s
Wall time: 512 ms

In [40]: len(compressed) / ind.nbytes  # compression ratio
Out[40]: 0.14296813539337488

In [41]: ind.nbytes / 0.512 / 1e9      # Compresson bandwidth (GB/s)
Out[41]: 2.654593515625

In [42]: %time _ = blosc.decompress(compressed)
CPU times: user 1.3 s, sys: 438 ms, total: 1.74 s
Wall time: 406 ms

In [43]: ind.nbytes / 0.406 / 1e9      # Decompression bandwidth (GB/s)
Out[43]: 3.3476647290640393
{% endhighlight %}

We store 7x fewer bytes on disk (thus septupling our effective disk I/O) by
adding an extra 3GB/s delay.  If we're on a really nice Macbook pro hard
drive (~600MB/s) then this is a clear and substantial win.  The worse the hard
drive, the better this is.

$$ \textrm{Effective bandwidth} = \left(\frac{1}{600MB/s \times 7} + \frac{1}{3000 MB/s}\right)^{-1} = 1750 MB/s$$


#### But floating point compression isn't as nice

Note however that some data is more or less compressable than others.
Consider this column of floating point data.

{% highlight Python %}
In [44]: x = df.pickup_latitude.compute().values
In [45]: %time compressed = blosc.compress_ptr(x.ctypes.data, len(x), x.dtype.alignment, clevel=5)
CPU times: user 5.87 s, sys: 0 ns, total: 5.87 s
Wall time: 925 ms

In [46]: len(compressed) / x.nbytes
Out[46]: 0.7518617315969132
{% endhighlight %}

Our floating point data compresses more slowly and only provides marginal
compression.  Compression may still be worth it on slow disk (perhaps at lower
compression levels) but this isn't a huge win.

#### Compression rules of thumb

Optimal compression requires thought.  General rules of thumb include the
following:

*  Compress integer dtypes
*  Compress datetimes
*  If your data is slowly varying (e.g. sorted time series) then use a shuffle filter (default in blosc)
*  Don't bother much with floating point dtypes
*  Compress categoricals (which are just integer dtypes)

#### Avoid gzip and bz2

Finally, avoid gzip and bz2.  These are both very common and *very* slow.  If
dealing with text data, consider [snappy](https://github.com/google/snappy)
(also available via blosc.)


### Indexing/Partitioning

One column usually dominates our queries.  In time-series data this is time.
For personal data this is the user ID.  Just as column stores let us avoid
reading irrelevant columns, partitioning our data along a preferred index
column lets us avoid reading irrelevant rows.  We might only want the data for
the last month and don't want to read several years' worth.  We might only need
the information for Alice and not Bob.

Traditional relational databases provide indexes on any number of columns or
sets of columns.  This is excellent if you are using a traditional relational
database.  Unfortunately the data structures to provide arbitrary indexes
don't mix well with some of the attributes discussed above and we're limited to
a single index that partitions our data into sorted blocks.


Some projects that implement these principles
---------------------------------------------

Many modern distributed database storage systems designed for analytic queries
implement these principles well.  Notable players include
[Redshift](http://docs.aws.amazon.com/redshift/latest/dg/welcome.html) and
[Parquet](https://github.com/Parquet/parquet-format).

Additionally newer single-machine data stores like [Dato's
SFrame](https://dato.com/products/create/docs/generated/graphlab.SFrame.html)
and [BColz](http://bcolz.blosc.org/) follow many of these principles.  Finally
many people have been doing this for a long time with custom use of libraries
like HDF5.

The rest of this post will talk about a tiny project,
[Castra](https://github.com/blaze/castra), that implements these princples and
gets good speedups on biggish Pandas data.


Castra
------

With these goals in mind we built [Castra](https://github.com/blaze/castra), a
binary partitioned compressed columnstore with builtin support for categoricals
and integration with both Pandas and dask.dataframe.

### Load data from CSV files, sort on index, save to Castra

Here we load in our data from CSV files, sort on the pickup datetime column,
and store to a castra file.  This takes about an hour (as compared to eleven
minutes for a single read.)

{% highlight Python %}
>>> df = dd.read_csv('csv/trip_data_*.csv',
...                  skipinitialspace=True,
...                  parse_dates=['pickup_datetime', 'dropoff_datetime'])

>>> (df.set_index('pickup_datetime', compute=False)
...    .to_castra('trip.castra', categories=True))
{% endhighlight %}


### Profit

Now we can take advantage of columnstores, compression, and binary
representation to perform analytic queries quickly.  Here is a plot of counts
of trip distance.

<img src="{{ BASE_PATH }}/images/dask-trip-distance.gif">

Note that this is especially fast because [Pandas now releases the
GIL](http://continuum.io/blog/pandas-releasing-the-gil) on `value_counts`
operations (all `groupby` operations really).  This takes around 20 seconds on
my machine on the last release of Pandas vs 5 seconds on the development
branch.  Moving from CSV files to Castra moved the bottleneck of our
computation from disk I/O to processing speed, allowing improvements like
multi-core processing to really shine.

We plot the result of the above computation with Bokeh below.  Note the spike
around 20km.  This is around the distance from Midtown Manhattan to LaGuardia
airport.

<iframe
src="https://cdn.rawgit.com/mrocklin/e269d02ed15947ba58c5/raw/2c8620c11af14fcdd6e0948de1d6b398cf56001a/nyc-trip-distance.html"
        width="830" height="350"></iframe>

### Look at the docs

I've shown Castra used above with
[dask.dataframe](http://dask.pydata.org/en/latest/dataframe.html) but it works
fine with straight Pandas too.


### Credit

Castra was started by myself and Valentin Haenel (current maintainer of
[bloscpack](https://github.com/blosc/bloscpack/) and
[bcolz](http://bcolz.blosc.org/)) during an evening sprint following PyData
Berlin.  Several bugfixes and refactors were followed up by Phil Cloud and Jim
Crist.  Castra is roughly 400 lines long.
